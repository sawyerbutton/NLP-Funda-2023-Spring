{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9ENLU90WGl",
        "outputId": "c24b69e3-7a4b-4ae8-ba3d-d77dca3c38c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFvBLJV0Dkv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-42fh0hjsF"
      },
      "source": [
        "## 导入数据集\n",
        "\n",
        "将数据集使用 Pandas 导入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoj29J24hPX"
      },
      "source": [
        "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVE3waNhuNj"
      },
      "source": [
        "为缩减训练时间,只采用前2000个数据\n",
        "你可以选择更多的数据进行验证"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTM3hOHW4hUY"
      },
      "source": [
        "batch_1 = df[:2000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRc2L89hh1Tf"
      },
      "source": [
        "查看原始数据-2000条数据-集中有0和1标签的分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvcfcCP5xpZ",
        "outputId": "a73003de-6477-4bd5-8fb5-8a37cde6af37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_1[1].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1041\n",
              "0     959\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MO08_KiAOb"
      },
      "source": [
        "## 加载预训练好的 Bert 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1InADgf5xm2",
        "outputId": "db0ad22d-7111-4910-e22a-fafdecf89a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 对于 DistilBERT:\n",
        "# 首先设定要使用的模型类别、分词器类别以及预训练模型的权重名称\n",
        "# 这里选择DistilBertModel（模型类别），DistilBertTokenizer（分词器类别），以及'distilbert-base-uncased'（预训练模型的权重名称）\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## 想要使用BERT就取消以下行的注释：\n",
        "# 这一行代码是为了如果你更喜欢使用原始的BERT模型而设定的，如果你取消这行代码的注释，那么你将加载BERT模型而不是DistilBERT模型\n",
        "# 记得注释上面那一行DistilbertModel的代码\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# 加载预训练模型/分词器\n",
        "# 从预训练的模型权重中加载分词器和模型。，\"from_pretrained\"函数可以直接加载预训练的模型和分词器\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDBMn3wiSX6"
      },
      "source": [
        "## Model #1: 经典数据预处理流程\n",
        "\n",
        "### Tokenization\n",
        "第一步将句子转化为words，在转化为subwords，确保能满足Bert模型的输入要求"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN"
      },
      "source": [
        "# 使用预训练的分词器对数据进行分词\n",
        "# batch_1[0]表示我们要处理的数据\n",
        "# \"apply\"函数用于对数据中的每个元素进行操作，这里的操作是一个lambda函数 aka 匿名函数\n",
        "# 在这个匿名函数中，调用了\"tokenizer.encode\"函数，对每个元素（这里是每个文本）进行分词。\n",
        "# \"add_special_tokens=True\"表示在分词的过程中添加特殊的标记，包括开始和结束的标记等，这些不能少，Bert模型 requirements\n",
        "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwjUwYgi-uL"
      },
      "source": [
        "\n",
        "### Padding\n",
        "\n",
        "文本预处理的经典步骤之-Padding 填充，确保输入的token长度是相同的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP"
      },
      "source": [
        "# 寻找最大的序列长度\n",
        "# 首先要找出所有分词后的句子中最长的句子的长度,创建一个变量max_len，并将其初始值设为0，然后遍历所有的句子，如果一个句子的长度大于max_len，更新max_len的值\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "# 对序列进行填充\n",
        "# 然后对所有的句子进行填充，使它们的长度都等于max_len。填充的方式是在句子的后面添加0，直到句子的长度等于max_len\n",
        "# 用np.array来存储填充后的句子。每一个句子都是一个数组，所有的句子构成了一个二维数组\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdjg306wjjmL"
      },
      "source": [
        "Our dataset is now in the `padded` variable, we can view its dimensions below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi7uXo95xeq",
        "outputId": "be786022-e84f-4e28-8531-0143af2347bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZBsYSDjzDV"
      },
      "source": [
        "### Masking\n",
        "\n",
        "对于基于Encoder 构建的 Bert，需要对 Attention 进行mask处理，确保Padding 出来的0不会对 Attention 过程产生side effects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_iGRNa_Ozc",
        "outputId": "206efe2e-a9e0-4bb4-d354-41f1576612ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 创建注意力掩码\n",
        "# 注意力掩码用于指示模型哪些位置的输入是真正的词语，哪些位置是填充的部分\n",
        "# 创建一个与padded数组形状相同的attention_mask数组，该数组中，padded中每个非零位置（即实际词语的位置）对应的值为1，零位置（即填充的位置）对应的值为0\n",
        "# np.where函数的作用是根据条件选择元素。条件是padded不等于0，如果条件为真（即位置上的元素不为0，是实际的词语），则选择1，否则选择0\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "attention_mask.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-CQB9-kN99"
      },
      "source": [
        "将与处理好的数据送给模型进行处理，得到模型的输出结果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UVjAV56PJz"
      },
      "source": [
        "# 将输入数据和注意力掩码转化为PyTorch张量\n",
        "# PyTorch的模型需要使用PyTorch的张量作为输入，所以需要将padded和attention_mask从NumPy数组转化为PyTorch张量 - required\n",
        "# 使用torch.tensor函数完成转化\n",
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "# 使用模型进行前向传播\n",
        "# 使用with torch.no_grad()语句块来锁定梯度计算\n",
        "# 在进行前向传播时，并不需要计算梯度，关闭梯度计算可以节省内存，提高计算速度\n",
        "# 然后，将input_ids和attention_mask作为输入，通过模型进行前向传播，得到输出last_hidden_states\n",
        "# attention_mask=attention_mask将attention_mask作为名为\"attention_mask\"的参数传递给模型\n",
        "# 这段代码会基于你的GPU的能力变化训练时间，如果训练时间过长，那就把前面的数据集再改小一点，2000变1000， 1000 变 500\n",
        "# 我大概跑了4分钟，希望能作为一个基准\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoCep_WVuB3v"
      },
      "source": [
        "\n",
        "因为在Bert模型中，输入句子转换后的数据的第一个位置是 [cls]，cls 代表的就是整个句子的 Sentence Encoding 或者说是 Sentence presentation，而作为下游的判别任务的依据就是这个 cls token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9t60At16PVs"
      },
      "source": [
        "# 提取特征\n",
        "# 从last_hidden_states（模型的输出，包含了每个输入词语的隐藏状态）中提取特征\n",
        "# last_hidden_states[0]取出了隐藏状态，因为模型的输出是一个元组，其第一个元素是隐藏状态\n",
        "# [:,0,:]表示取出每个句子的第一个词语（BERT模型中的CLS标记）的所有隐藏状态\n",
        "# 在BERT模型中，第一个词语的隐藏状态被用作句子的表示，所以我们通常会使用它作为特征\n",
        "# .numpy()将隐藏状态从PyTorch张量转化为NumPy数组，因为我们接下来可能会使用NumPy或者其他库来处理这些特征\n",
        "# 因为后续会将这个特征送给一个NN来处理\n",
        "features = last_hidden_states[0][:,0,:].numpy()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZVU66Gurr-"
      },
      "source": [
        "Batch中的[1] 代表真实的标签，0或1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3fX2yh6PTx"
      },
      "source": [
        "labels = batch_1[1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoEvM2evRx1"
      },
      "source": [
        "## Model #2: Train/Test Split\n",
        "\n",
        "将数据切分成训练集和测试集，直接使用API切分了，省事"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAqbkoU6PP9"
      },
      "source": [
        "# 划分训练集和测试集\n",
        "# 使用sklearn库的train_test_split函数将数据划分为训练集和测试集\n",
        "# \"features\"是刚刚计算的到的特征，\"labels\"是训练集的标签\n",
        "# 函数返回的四个值分别是训练集特征、测试集特征、训练集标签和测试集标签\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCT9u8vAwnID"
      },
      "source": [
        "训练最后的逻辑回归模型的参数\n",
        "虽然Bert是预训练过的，但是在做下游任务时的模型是没有训练的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-EVWx4CzBc",
        "outputId": "2fabd878-c2a2-470a-b5d1-443ef9f61aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# 创建逻辑回归模型\n",
        "# 使用sklearn库的LogisticRegression类创建了一个逻辑回归模型\n",
        "lr_clf = LogisticRegression()\n",
        "\n",
        "# 训练逻辑回归模型\n",
        "# 使用fit方法来训练模型\n",
        "# fit方法需要两个参数：训练集的特征和训练集的标签，在上一步得到了\n",
        "lr_clf.fit(train_features, train_labels)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rUMKuVgwzkY"
      },
      "source": [
        "计算训练效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoyxRJ7ECTA",
        "outputId": "5df8a560-3788-42c2-adb9-03ff2580ee70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 评估模型\n",
        "# 使用score方法来评估模型在测试集上的表现\n",
        "# score方法需要两个参数：测试集的特征和测试集的标签\n",
        "# score方法会返回模型在测试集上的准确率，即正确预测的样本数占总样本数的比例, 体现模型在真实场景中的能力\n",
        "lr_clf.score(test_features, test_labels)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75oyhr3VxHoE"
      },
      "source": [
        "只得到自己的得分其实很不合理，考试都得和别人比\n",
        "这里使用一个 dummy classifier （模拟分类器/或者叫傻瓜分类器）来进行分类，比较得分\n",
        "Dummy 是不会进行任何学习的，只是简单地使用一些规则（如预测所有样本都属于最常见的类别）来进行预测，比如使用众数进行分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnwgmqNG7i5l",
        "outputId": "7a81ffc3-265c-4498-9ec9-26cb8601ffe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 导入DummyClassifier\n",
        "# DummyClassifier是一种简单的分类器，它不进行任何学习，只是简单地使用一些规则（如预测所有样本都属于最常见的类别）来进行预测\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# 创建Dummy分类器\n",
        "clf = DummyClassifier()\n",
        "\n",
        "# 使用交叉验证来评估模型的性能\n",
        "# cross_val_score函数会将数据集划分为k个子集，然后进行k次训练和测试\n",
        "# 每次，都会选择一个子集作为测试集，其他的子集作为训练集\n",
        "# cross_val_score函数返回的是每次测试的分数（在这里是准确率）\n",
        "# 计算这些分数的平均值和标准差来评估模型的性能\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "\n",
        "# Dummy分类器的平均分数和95%置信区间（即平均分数±两倍的标准差）的方式来进行比较\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy classifier score: 0.511 (+/- 0.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.8 大于 0.5  只能说这个得分比傻子好点，\n",
        "- 事实上我们如果增加数据集的大小，\n",
        "- 将固定参数的方式改为  fine tunning 的方式进行会更好\n",
        "- 但那也会消耗更多的时间进行训练\n",
        "\n",
        "在本demo中不进行展示了"
      ],
      "metadata": {
        "id": "J4q-XEzf9tSK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQuqV6cnWQu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}