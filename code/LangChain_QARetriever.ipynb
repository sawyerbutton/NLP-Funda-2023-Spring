{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWbeHuDF2WzS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "metadata": {
        "id": "yWlPMtXD2hOK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "ibCy_Aks23wH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
      ],
      "metadata": {
        "id": "if6AHfB0cYPy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "# from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = UnstructuredFileLoader(\"NER.txt\")\n",
        "document = loader.load()\n",
        "print(f'documents:{len(document)}')\n",
        "# text_splitter = RecursiveCharacterTextSplitter(separators=['。']).from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
        "separators = [\"。\", \" \"]\n",
        "# text_splitter = RecursiveCharacterTextSplitter(separators=separators, chunk_size=1000, chunk_overlap=0)\n",
        "text_splitter = RecursiveCharacterTextSplitter(separators=separators, chunk_size=500, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(document)\n",
        "print(f'chunks:{len(texts)}')\n",
        "# print(texts)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "llm = OpenAI(temperature=0,verbose = True, model='gpt-3.5-turbo')\n",
        "# _filter = LLMChainFilter.from_llm(llm)\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.81)\n",
        "# pipeline_compressor = DocumentCompressorPipeline(\n",
        "#     transformers=[_filter]\n",
        "# )\n",
        "# docsearch = Chroma.from_documents(texts, embeddings)\n",
        "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=relevant_filter, base_retriever=retriever)\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"本发明方法和系统的有益效果有哪些\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "id": "t9d7mrUI2rNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.retrievers import ContextualCompressionRetriever\n",
        "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# llm = OpenAI(temperature=0)\n",
        "# compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "# compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
        "# compressed_docs = compression_retriever.get_relevant_documents(\"本发明的有益效果有哪些\")\n",
        "# pretty_print_docs(compressed_docs)\n"
      ],
      "metadata": {
        "id": "dauFZ3azWx39"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template1 = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案就回答不知道，不要编造答案.\n",
        "\n",
        "{context}\n",
        "\n",
        "问题: {question}\n",
        "请用正式的中文回答，注意不要出现语病和错别字:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template1, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT,'verbose': True}\n",
        "# compression_retriever 当前并不支持 RetrieverQA 的 retriever，只能使用base vectorRetriever 实现\n",
        "# issue https://github.com/hwchase17/langchain/issues/7168\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI( verbose=True), chain_type=\"stuff\", retriever=retriever ,return_source_documents=True, chain_type_kwargs=chain_type_kwargs)\n"
      ],
      "metadata": {
        "id": "mlTWuv1-2yGx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"替换你的问题\"\n",
        "result = qa({\"query\": query})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "CG1mRZx84v3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LimA-vbRple"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}