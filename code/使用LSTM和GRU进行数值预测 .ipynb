{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "puVBBHHmdnLg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REw6DS0mj4b0",
    "outputId": "45d301eb-c1af-4a2d-99f9-90df34b976bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['使用LSTM和GRU进行数值预测.ipynb', '.ipynb_checkpoints', 'PJM_Load_hourly.csv', 'NI_hourly.csv', 'FE_hourly.csv', 'EKPC_hourly.csv', 'PJMW_hourly.csv', 'PJME_hourly.csv', 'est_hourly.paruqet', 'DEOK_hourly.csv', 'DUQ_hourly.csv', 'DOM_hourly.csv', 'COMED_hourly.csv', 'DAYTON_hourly.csv', 'AEP_hourly.csv', 'pjm_hourly_est.csv']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RtIV0AJ-hX3h",
    "outputId": "3f9bffe4-2b6a-496b-df89-cb783c78215c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>AEP_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-12-31 01:00:00</td>\n",
       "      <td>13478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-12-31 02:00:00</td>\n",
       "      <td>12865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-12-31 03:00:00</td>\n",
       "      <td>12577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-12-31 04:00:00</td>\n",
       "      <td>12517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-12-31 05:00:00</td>\n",
       "      <td>12670.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime   AEP_MW\n",
       "0  2004-12-31 01:00:00  13478.0\n",
       "1  2004-12-31 02:00:00  12865.0\n",
       "2  2004-12-31 03:00:00  12577.0\n",
       "3  2004-12-31 04:00:00  12517.0\n",
       "4  2004-12-31 05:00:00  12670.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.read_csv('AEP_hourly.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dzlCV6lmsV6"
   },
   "source": [
    "- 我们共有12个.csv文件，其中包含小时能源趋势数据（'est_hourly.paruqet'和'pjm_hourly_est.csv' 这两个文件没啥用）\n",
    "- 在接下来的步骤中，我们将按照以下顺序读取这些文件并预处理这些数据：\n",
    "\n",
    "- 获取每个单独时间步的时间数据并将其归纳为：\n",
    "\n",
    "  - 一天中的小时，即 0-23\n",
    "  - 一周中的天数，即 1-7\n",
    "  - 月份，即 1-12\n",
    "  - 年中的天数，即 1-365\n",
    "\n",
    "- 将数据缩放到0和1之间的值\n",
    "  - 当特征在相对相似的范围内和/或接近正态分布时，算法往往表现更好或收敛更快\n",
    "  - 缩放保留了原始分布的形状，不会降低异常值的重要性\n",
    "\n",
    "- 将数据分组为模型输入的序列，并存储相应的标签：\n",
    "\n",
    "  - 序列长度或回溯期是模型用来进行预测的历史数据点的数量\n",
    "  - 标签将是输入序列中最后一个数据点之后的下一个时间点的数据\n",
    "- 将输入和标签分割为训练和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X3cZQnyNuP4u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-aef9620a7f48>:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(os.listdir(data_dir)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fe3acd3b594f14a738dc389293b31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 缩放器对象将存储在此字典中，以便在评估期间可以对模型的输出测试数据进行重新缩放\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "# 使用tqdm_notebook遍历数据目录中的文件\n",
    "for file in tqdm_notebook(os.listdir(data_dir)):\n",
    "# 跳过我们不使用的文件\n",
    "  if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
    "    continue\n",
    "  # 将csv文件存储在Pandas DataFrame中\n",
    "  df = pd.read_csv(data_dir + file, parse_dates=[0])\n",
    "  # 将时间数据处理为合适的输入格式\n",
    "  df['hour'] = df.apply(lambda x: x['Datetime'].hour, axis=1)\n",
    "  df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek, axis=1)\n",
    "  df['month'] = df.apply(lambda x: x['Datetime'].month, axis=1)\n",
    "  df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear, axis=1)\n",
    "  df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
    "\n",
    "  # 缩放输入数据\n",
    "  sc = MinMaxScaler()\n",
    "  label_sc = MinMaxScaler()\n",
    "  data = sc.fit_transform(df.values)\n",
    "  # 获取标签（使用数据）的缩放，以便在评估期间可以将输出重新缩放为实际值\n",
    "  label_sc.fit(df.iloc[:, 0].values.reshape(-1, 1))\n",
    "  label_scalers[file] = label_sc\n",
    "\n",
    "  # 定义回溯期并拆分输入/标签\n",
    "  lookback = 90\n",
    "  inputs = np.zeros((len(data) - lookback, lookback, df.shape[1]))\n",
    "  labels = np.zeros(len(data) - lookback)\n",
    "\n",
    "  for i in range(lookback, len(data)):\n",
    "      inputs[i - lookback] = data[i - lookback:i]\n",
    "      labels[i - lookback] = data[i, 0]\n",
    "  inputs = inputs.reshape(-1, lookback, df.shape[1])\n",
    "  labels = labels.reshape(-1, 1)\n",
    "\n",
    "  # 将数据拆分为训练/测试部分，并将来自不同文件的所有数据合并到单个数组中\n",
    "  test_portion = int(0.1 * len(inputs))\n",
    "  if len(train_x) == 0:\n",
    "      train_x = inputs[:-test_portion]\n",
    "      train_y = labels[:-test_portion]\n",
    "  else:\n",
    "      train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "      train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "  test_x[file] = (inputs[-test_portion:])\n",
    "  test_y[file] = (labels[-test_portion:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J9l5XpSTjX64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>AEP_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-12-31 01:00:00</td>\n",
       "      <td>13478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-12-31 02:00:00</td>\n",
       "      <td>12865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-12-31 03:00:00</td>\n",
       "      <td>12577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-12-31 04:00:00</td>\n",
       "      <td>12517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-12-31 05:00:00</td>\n",
       "      <td>12670.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime   AEP_MW\n",
       "0  2004-12-31 01:00:00  13478.0\n",
       "1  2004-12-31 02:00:00  12865.0\n",
       "2  2004-12-31 03:00:00  12577.0\n",
       "3  2004-12-31 04:00:00  12517.0\n",
       "4  2004-12-31 05:00:00  12670.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('AEP_hourly.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNIycmqcoUOL"
   },
   "source": [
    "我们共有980,185个训练数据序列\n",
    "\n",
    "为了提高训练速度，可以分批处理数据，这样模型就不需要频繁地更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ERSiPLRwk5E-"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Obkt0B0uk7GZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() 检查并返回一个布尔值True，表示是否有可用的GPU，否则返回False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# 如果我们有可用的GPU，我们将把设备设置为GPU。稍后我们将在代码中使用这个设备变量\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J1vVzYho1-D"
   },
   "source": [
    "定义GRU和LSTM模型的结构。\n",
    "- 两种模型具有相同的结构，唯一的区别在于循环层（GRU/LSTM）以及隐藏状态的初始化。\n",
    "- LSTM的隐藏状态是一个包含单元状态和隐藏状态的元组，而GRU只有一个隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZKCsHvo5k9gS"
   },
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 定义GRU层\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # 定义ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # 前向传播，GRU层输出结果和隐藏状态\n",
    "        out, h = self.gru(x, h)\n",
    "        # 将GRU层的输出传入全连接层\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # 初始化隐藏状态\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # 定义ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # 前向传播，LSTM层输出结果和隐藏状态\n",
    "        out, h = self.lstm(x, h)\n",
    "        # 将LSTM层的输出传入全连接层\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fYZ0FgMpFgn"
   },
   "source": [
    "- 下面的函数定义了训练过程，这样可以对两个模型进行重复操作。\n",
    "- 两个模型在隐藏状态的维数、层数、训练周期数和学习率方面都是相同的，并在完全相同的数据集上进行训练和测试\n",
    "\n",
    "- 为了比较两个模型的性能，我们还将跟踪模型的训练时间，并最终比较两个模型在测试集上的最终准确率。\n",
    "- 作为准确度衡量标准，使用对称平均绝对百分比误差（sMAPE）来评估模型\n",
    "- sMAPE是预测值与实际值之间的绝对差值之和除以预测值与实际值的平均值，从而得到一个百分比来衡量误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grm-AhdnlSuy"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    # 获取输入数据的维度\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "\n",
    "    # 选择使用GRU还是LSTM模型\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "\n",
    "    # 开始训练模型\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.process_time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            # 计算损失\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter % 200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.process_time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # 评估模型\n",
    "    for i in test_x.keys():\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.process_time()-start_time)))\n",
    "    sMAPE = 0\n",
    "\n",
    "    # 计算sMAPE\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v54an0XXt6zC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.005567154412856326\n",
      "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.0031107321189483626\n",
      "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.0022110680652743515\n",
      "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.0017360910079878523\n",
      "Epoch 1/5 Done, Total Loss: 0.001492845536226795\n",
      "Time Elapsed for Epoch: 207.51063615600003 seconds\n",
      "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.00021200361974479164\n",
      "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.00020321043080912205\n",
      "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.000197148703809944\n",
      "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.00019122636471365694\n",
      "Epoch 2/5 Done, Total Loss: 0.0001862149163698651\n",
      "Time Elapsed for Epoch: 206.89833430600004 seconds\n",
      "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.000149296151685121\n",
      "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00014898897577950265\n",
      "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.00014681770451716147\n",
      "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.0001433243802694051\n",
      "Epoch 3/5 Done, Total Loss: 0.00014242030163609415\n",
      "Time Elapsed for Epoch: 206.42224692000002 seconds\n",
      "Epoch 4......Step: 200/957....... Average Loss for Epoch: 0.00012797945633792553\n",
      "Epoch 4......Step: 400/957....... Average Loss for Epoch: 0.00012655253638513386\n",
      "Epoch 4......Step: 600/957....... Average Loss for Epoch: 0.00012736728635597198\n",
      "Epoch 4......Step: 800/957....... Average Loss for Epoch: 0.0001246917989828944\n",
      "Epoch 4/5 Done, Total Loss: 0.0001233710425437789\n",
      "Time Elapsed for Epoch: 206.6868841270001 seconds\n",
      "Epoch 5......Step: 200/957....... Average Loss for Epoch: 0.00011543403612449765\n",
      "Epoch 5......Step: 400/957....... Average Loss for Epoch: 0.00011503438277941313\n",
      "Epoch 5......Step: 600/957....... Average Loss for Epoch: 0.0001110777850044542\n",
      "Epoch 5......Step: 800/957....... Average Loss for Epoch: 0.00011134655882415245\n",
      "Epoch 5/5 Done, Total Loss: 0.00011084277993345452\n",
      "Time Elapsed for Epoch: 206.85589879600002 seconds\n",
      "Total Training Time: 1034.3740003050002 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KX7WqEOlchr"
   },
   "outputs": [],
   "source": [
    "lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8LRp7j4lgI3"
   },
   "outputs": [],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cEbpP7Nlgk3"
   },
   "outputs": [],
   "source": [
    "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmsTr17GtOm8"
   },
   "source": [
    "- 尽管LSTM模型可能产生较小的误差，并在性能准确性方面略胜于GRU模型，但这种差异并不显著，因此无法得出结论。\n",
    "- 比较这两种模型的测试，但总体上并没有一个明确结果说明LSTM和GRU的孰优孰劣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGzBHTyatW85"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
