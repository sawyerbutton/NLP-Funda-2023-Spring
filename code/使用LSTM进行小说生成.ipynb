{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGqMHYXDS_3t",
        "outputId": "62dd8716-f644-42c4-92a2-68fd48e776de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  164014\n",
            "Total Vocab:  63\n",
            "Total Patterns:  163914\n",
            "Epoch 0: Cross-entropy: 438310.6250\n",
            "Epoch 1: Cross-entropy: 404915.1875\n",
            "Epoch 2: Cross-entropy: 378032.7188\n",
            "Epoch 3: Cross-entropy: 358118.6562\n",
            "Epoch 4: Cross-entropy: 340549.2500\n",
            "Epoch 5: Cross-entropy: 326148.2812\n",
            "Epoch 6: Cross-entropy: 315271.2812\n",
            "Epoch 7: Cross-entropy: 304081.3750\n",
            "Epoch 8: Cross-entropy: 295791.5000\n",
            "Epoch 9: Cross-entropy: 288530.8438\n",
            "Epoch 10: Cross-entropy: 285020.3750\n",
            "Epoch 11: Cross-entropy: 274777.7188\n",
            "Epoch 12: Cross-entropy: 267050.9062\n",
            "Epoch 13: Cross-entropy: 263274.4375\n",
            "Epoch 14: Cross-entropy: 259068.4688\n",
            "Epoch 15: Cross-entropy: 253655.7656\n",
            "Epoch 16: Cross-entropy: 248130.1562\n",
            "Epoch 17: Cross-entropy: 245956.7500\n",
            "Epoch 18: Cross-entropy: 240772.8594\n",
            "Epoch 19: Cross-entropy: 238264.5781\n",
            "Epoch 20: Cross-entropy: 235628.5000\n",
            "Epoch 21: Cross-entropy: 232212.8125\n",
            "Epoch 22: Cross-entropy: 226408.9531\n",
            "Epoch 23: Cross-entropy: 225582.9219\n",
            "Epoch 24: Cross-entropy: 220097.8281\n",
            "Epoch 25: Cross-entropy: 218975.8125\n",
            "Epoch 26: Cross-entropy: 218730.5312\n",
            "Epoch 27: Cross-entropy: 213845.4844\n",
            "Epoch 28: Cross-entropy: 500290.0312\n",
            "Epoch 29: Cross-entropy: 468392.2812\n",
            "Epoch 30: Cross-entropy: 458800.0000\n",
            "Epoch 31: Cross-entropy: 449297.9062\n",
            "Epoch 32: Cross-entropy: 439321.7500\n",
            "Epoch 33: Cross-entropy: 431440.7812\n",
            "Epoch 34: Cross-entropy: 423548.6250\n",
            "Epoch 35: Cross-entropy: 417446.9375\n",
            "Epoch 36: Cross-entropy: 410486.6250\n",
            "Epoch 37: Cross-entropy: 405079.7500\n",
            "Epoch 38: Cross-entropy: 397673.9062\n",
            "Epoch 39: Cross-entropy: 393231.4062\n",
            "Prompt: \"l, certainly,” said alice, looking down with wonder\n",
            "at the mouse’s tail; “but why do you call it sad\"\n",
            "d the harten the harten the harten the harten the harten the harten the harten the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter the harter the harter the harter the car the car the car the car the car the harter the harter the harter \n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "# 1. 读取并处理输入文件，将文本全部转换为小写\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# 2. 创建字符到整数的映射\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "# 3. 输出文本的总字符数和独特字符数\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "# 4. 准备数据集，根据设定的序列长度（seq_length）将输入文本转换为整数编码的输入输出对\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "# 5. 转换并标准化输入数据为PyTorch张量\n",
        "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(dataY)\n",
        "\n",
        "# 6. 定义一个字符级LSTM模型，其中包括一个LSTM层、一个Dropout层以及一个全连接层\n",
        "class CharModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x\n",
        "\n",
        "# 7. 设置训练参数：迭代次数（n_epochs）和批量大小（batch_size）\n",
        "n_epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "# 8. 初始化模型并将其移到GPU上（如果可用）\n",
        "model = CharModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 9. 选择优化器（Adam）和损失函数（交叉熵损失）\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# 10. 训练模型，同时记录最佳模型状态\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # 11. 验证模型性能，如果性能更好，记录最佳模型\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
        "\n",
        "# 12. 保存最佳模型状态和字符到整数的映射\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")\n",
        "\n",
        "# 13. 加载保存的模型状态和字符到整数的映射\n",
        "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 14. 随机选择一个文本片段作为生成文本的初始提示\n",
        "filename = \"wonderland.txt\"\n",
        "seq_length = 100\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n",
        "\n",
        "# 15. 使用训练好的模型生成文本\n",
        "model.eval()\n",
        "print('Prompt: \"%s\"' % prompt)\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        prediction = model(x.to(device))\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rizQjtPwVixW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}